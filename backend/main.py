import json
import re
import traceback
import time
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from catboost import CatBoostRegressor
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# ==========================================
# Load Model
# ==========================================
model = CatBoostRegressor()
model.load_model("catboost_price_model_final.cbm")

with open("model_features.json", "r", encoding="utf-8") as f:
    feature_list = json.load(f)

with open("categorical_features.json", "r", encoding="utf-8") as f:
    categorical_features = json.load(f)

# ==========================================
# FastAPI Setup
# ==========================================
app = FastAPI(
    title="Real Estate Price Prediction API",
    version="5.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# Schemas
# ==========================================
class ApartmentInput(BaseModel):
    data: dict

class LinkInput(BaseModel):
    url: str


# ==========================================
# Prepare Model Input
# ==========================================
def prepare_input_dataframe(input_dict: dict) -> pd.DataFrame:
    for feature in feature_list:
        if feature not in input_dict:
            input_dict[feature] = None

    df = pd.DataFrame([input_dict])
    df = df[feature_list]

    for col in df.columns:
        if col in categorical_features:
            df[col] = df[col].astype(str)
        else:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.fillna(0)
    return df


# ==========================================
# Setup Selenium Driver
# ==========================================
def setup_driver():
    """Configure Chrome to look like a real user"""
    options = Options()
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument('--headless')  # Run in headless mode for server
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver


# ==========================================
# Map Russian Fields to English Model Features
# ==========================================
def map_russian_to_english(russian_data: dict) -> dict:
    """Convert Russian field names to English model feature names"""
    
    field_mapping = {
        '–†–∞–π–æ–Ω': 'District',
        '–ö–ª–∞—Å—Å –ñ–∏–ª—å—è': 'Class',
        '–ö–æ—Ä–ø—É—Å': 'Building',
        '–í—Å–µ–≥–æ –≠—Ç–∞–∂–µ–π': 'FloorsTotal',
        '–û—á–µ—Ä–µ–¥—å': 'Phase',
        '–¢–∏–ø –ó–¥–∞–Ω–∏—è': 'BuildingType',
        '–≠—Ç–∞–∂': 'Floor',
        '–°–µ–∫—Ü–∏—è': 'Section',
        '–¢–∏–ø –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏': 'PropertyType',
        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': 'PropertyCategory',
        '–ö–≤–∞—Ä—Ç–∏—Ä—ã': 'Apartments',
        '–û—Ç–¥–µ–ª–∫–∞': 'Finishing',
        '–°—Ç–∞—Ç—É—Å': 'Status',
        '–í–∞—Ä–∏–∞–Ω—Ç –ö–≤.': 'ApartmentOption',
        '–ò–ø–æ—Ç–µ–∫–∞': 'Mortgage',
        '–°—É–±—Å–∏–¥–∏–∏': 'Subsidies',
        '–ü–ª–∞–Ω–∏—Ä–æ–≤–∫–∞': 'Layout',
        '–í—ã—Å–æ—Ç–∞ –ü–æ—Ç–æ–ª–∫–æ–≤': 'CeilingHeight',
        '–û–±—â–∞—è –ü–ª–æ—â–∞–¥—å': 'TotalArea',
        '–ü–ª–æ—â–∞–¥—å –±–µ–∑ –ë–∞–ª–∫–æ–Ω–∞': 'AreaWithoutBalcony',
        '–ñ–∏–ª–∞—è –ü–ª–æ—â–∞–¥—å': 'LivingArea',
        '–ü–ª–æ—â–∞–¥—å –ö—É—Ö–Ω–∏': 'KitchenArea',
        '–ü–ª–æ—â–∞–¥—å –ö–æ—Ä–∏–¥–æ—Ä–∞': 'HallwayArea',
        '–ü–ª–æ—â–∞–¥—å –í–∞–Ω–Ω–æ–π': 'BathroomArea',
        '–ü–ª–æ—â–∞–¥—å –ë–∞–ª–∫–æ–Ω–∞': 'BalconyArea',
        '–ü–ª–æ—â–∞–¥—å –£—á–∞—Å—Ç–∫–∞': 'PlotArea',
        '–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ (–ö–æ–¥)': 'Developer_encoded',
        '–ö–æ–º–ø–ª–µ–∫—Å (–ö–æ–¥)': 'Complex_encoded'
    }
    
    english_data = {}
    for rus_key, rus_value in russian_data.items():
        eng_key = field_mapping.get(rus_key, rus_key)
        
        # Clean numeric values (remove –º¬≤, convert to float)
        if eng_key in ['TotalArea', 'AreaWithoutBalcony', 'LivingArea', 'KitchenArea', 
                       'HallwayArea', 'BathroomArea', 'BalconyArea', 'PlotArea', 'CeilingHeight']:
            # Extract number from "85 –º¬≤" or "85.5 –º¬≤"
            match = re.search(r'(\d+[.,]?\d*)', str(rus_value))
            if match:
                english_data[eng_key] = float(match.group(1).replace(',', '.'))
        elif eng_key in ['Floor', 'FloorsTotal']:
            # Extract just the number
            match = re.search(r'(\d+)', str(rus_value))
            if match:
                english_data[eng_key] = int(match.group(1))
        else:
            english_data[eng_key] = rus_value
    
    return english_data


# ==========================================
# Extract Apartment Data Using 3-Tier Strategy (UPGRADED)
# ==========================================
def extract_apartment_data(url: str) -> dict:
    """Extract apartment data using upgraded 3-tier strategy from cian_scraper.py"""
    driver = setup_driver()
    all_data = {}
    
    try:
        print(f"\nüöÄ Loading: {url}")
        driver.get(url)
        time.sleep(5)  # Wait for JavaScript to load
        
        # ===== METHOD 1: JAVASCRIPT JSON EXTRACTION (THE UPGRADE) =====
        print("üîç Method 1: Extracting internal JSON data...")
        try:
            json_data = driver.execute_script("""
                if (window.__NEXT_DATA__) return window.__NEXT_DATA__;
                if (window._cianConfig) return window._cianConfig;
                
                const elements = document.querySelectorAll('[data-props]');
                for (let el of elements) {
                    try { return JSON.parse(el.getAttribute('data-props')); } catch(e) {}
                }
                
                const scripts = document.querySelectorAll('script[type="application/json"]');
                for (let script of scripts) {
                    try {
                        const data = JSON.parse(script.innerText);
                        if (data && (data.offer || data.apartment || data.building)) return data;
                    } catch(e) {}
                }
                
                if (window.__INITIAL_STATE__) return window.__INITIAL_STATE__;
                return null;
            """)
            
            if json_data:
                # Extract from JSON blob with comprehensive patterns
                json_str = json.dumps(json_data, ensure_ascii=False)
                
                patterns = {
                    '–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ (–ö–æ–¥)': [r'"developer(?:Name)?"\s*:\s*"([^"]+)"', r'"builder"\s*:\s*"([^"]+)"'],
                    '–ö–æ–º–ø–ª–µ–∫—Å (–ö–æ–¥)': [r'"complex(?:Name)?"\s*:\s*"([^"]+)"', r'"residentialComplex"\s*:\s*"([^"]+)"'],
                    '–ö–ª–∞—Å—Å –ñ–∏–ª—å—è': [r'"buildingClass"\s*:\s*"([^"]+)"', r'"class"\s*:\s*"([^"]+)"'],
                    '–í—Å–µ–≥–æ –≠—Ç–∞–∂–µ–π': [r'"floorsTotal"\s*:\s*(\d+)', r'"totalFloors"\s*:\s*(\d+)'],
                    '–í—ã—Å–æ—Ç–∞ –ü–æ—Ç–æ–ª–∫–æ–≤': [r'"ceilingHeight"\s*:\s*([\d\.]+)'],
                    '–¢–∏–ø –ó–¥–∞–Ω–∏—è': [r'"buildingType"\s*:\s*"([^"]+)"', r'"material"\s*:\s*"([^"]+)"'],
                    '–û—Ç–¥–µ–ª–∫–∞': [r'"finishing"\s*:\s*"([^"]+)"', r'"renovation"\s*:\s*"([^"]+)"'],
                    '–ü–ª–∞–Ω–∏—Ä–æ–≤–∫–∞': [r'"planning"\s*:\s*"([^"]+)"', r'"layout"\s*:\s*"([^"]+)"'],
                    '–°–µ–∫—Ü–∏—è': [r'"section"\s*:\s*"([^"]+)"'],
                    '–ö–æ—Ä–ø—É—Å': [r'"building"\s*:\s*"([^"]+)"', r'"corps"\s*:\s*"([^"]+)"'],
                    '–û—á–µ—Ä–µ–¥—å': [r'"phase"\s*:\s*"([^"]+)"', r'"stage"\s*:\s*"([^"]+)"'],
                    '–†–∞–π–æ–Ω': [r'"district"\s*:\s*"([^"]+)"', r'"area"\s*:\s*"([^"]+)"'],
                    '–≠—Ç–∞–∂': [r'"floor"\s*:\s*(\d+)'],
                    '–û–±—â–∞—è –ü–ª–æ—â–∞–¥—å': [r'"totalArea"\s*:\s*([\d\.]+)', r'"area"\s*:\s*([\d\.]+)'],
                    '–ñ–∏–ª–∞—è –ü–ª–æ—â–∞–¥—å': [r'"livingArea"\s*:\s*([\d\.]+)'],
                    '–ü–ª–æ—â–∞–¥—å –ö—É—Ö–Ω–∏': [r'"kitchenArea"\s*:\s*([\d\.]+)'],
                    '–ü–ª–æ—â–∞–¥—å –ë–∞–ª–∫–æ–Ω–∞': [r'"balconyArea"\s*:\s*([\d\.]+)'],
                    '–°—Ç–∞—Ç—É—Å': [r'"status"\s*:\s*"([^"]+)"'],
                    '–ò–ø–æ—Ç–µ–∫–∞': [r'"mortgage"\s*:\s*"([^"]+)"'],
                    '–°—É–±—Å–∏–¥–∏–∏': [r'"subsidy"\s*:\s*"([^"]+)"'],
                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': [r'"category"\s*:\s*"([^"]+)"'],
                    '–¢–∏–ø –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏': [r'"propertyType"\s*:\s*"([^"]+)"']
                }
                
                for field, pattern_list in patterns.items():
                    for pattern in pattern_list:
                        match = re.search(pattern, json_str, re.IGNORECASE)
                        if match:
                            value = next((g for g in match.groups() if g is not None), None)
                            if value:
                                all_data[field] = value.strip() if isinstance(value, str) else str(value)
                                break
                
                print(f"   ‚úÖ Found {len(all_data)} fields from JSON")
            else:
                print("   ‚ùå No JSON data found")
        except Exception as e:
            print(f"   ‚ùå JSON extraction error: {e}")
        
        # ===== METHOD 2: BEAUTIFULSOUP HTML PARSING =====
        print("üîç Method 2: Extracting HTML data...")
        try:
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            param_items = soup.select('li[class*="cui-wzd2b5"], div[class*="c6c5c8b1"], span[class*="c1c5b1a0"]')
            
            html_count = 0
            for item in param_items:
                text = item.get_text(strip=True)
                if not text or '\n' not in text:
                    continue
                    
                lines = text.split('\n')
                if len(lines) >= 2:
                    label, value = lines[0].strip(), lines[1].strip()
                    
                    if '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å' in label and '–û–±—â–∞—è –ü–ª–æ—â–∞–¥—å' not in all_data:
                        all_data['–û–±—â–∞—è –ü–ª–æ—â–∞–¥—å'] = value
                        html_count += 1
                    elif '–≠—Ç–∞–∂' in label and '–í—Å–µ–≥–æ' not in label and '–≠—Ç–∞–∂' not in all_data:
                        all_data['–≠—Ç–∞–∂'] = value
                        html_count += 1
                    elif '–≠—Ç–∞–∂–µ–π' in label and '–í—Å–µ–≥–æ' in label and '–í—Å–µ–≥–æ –≠—Ç–∞–∂–µ–π' not in all_data:
                        all_data['–í—Å–µ–≥–æ –≠—Ç–∞–∂–µ–π'] = value
                        html_count += 1
                    elif '–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫' in label and '–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ (–ö–æ–¥)' not in all_data:
                        all_data['–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ (–ö–æ–¥)'] = value
                        html_count += 1
                    elif ('–ö–æ–º–ø–ª–µ–∫—Å' in label or '–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å' in label) and '–ö–æ–º–ø–ª–µ–∫—Å (–ö–æ–¥)' not in all_data:
                        all_data['–ö–æ–º–ø–ª–µ–∫—Å (–ö–æ–¥)'] = value
                        html_count += 1
            
            print(f"   ‚úÖ Found {html_count} additional fields from HTML")
        except Exception as e:
            print(f"   ‚ùå HTML extraction error: {e}")
        
        # ===== METHOD 3: PAGE TEXT REGEX =====
        print("üîç Method 3: Scanning page text...")
        try:
            page_text = driver.find_element("tag name", "body").text
            
            text_patterns = {
                '–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ (–ö–æ–¥)': r'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫[:\s]+([^\n]+)',
                '–ö–æ–º–ø–ª–µ–∫—Å (–ö–æ–¥)': r'–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å[:\s]+([^\n]+)',
                '–í—ã—Å–æ—Ç–∞ –ü–æ—Ç–æ–ª–∫–æ–≤': r'–í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤[:\s]+([^\n]+)',
                '–û–±—â–∞—è –ü–ª–æ—â–∞–¥—å': r'–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å[:\s]+([\d\.,]+\s*–º¬≤)'
            }
            
            text_count = 0
            for field, pattern in text_patterns.items():
                if field not in all_data:
                    match = re.search(pattern, page_text, re.IGNORECASE)
                    if match:
                        all_data[field] = match.group(1).strip()
                        text_count += 1
            
            print(f"   ‚úÖ Found {text_count} additional fields from text")
        except Exception as e:
            print(f"   ‚ùå Text extraction error: {e}")
        
        print(f"\nüìä EXTRACTION COMPLETE: {len(all_data)} total fields found")
        
        # Convert to English field names
        english_data = map_russian_to_english(all_data)
        
        print(f"\n=== Mapped to English Model Features ===")
        for key, value in english_data.items():
            print(f"  ‚úì {key}: {value}")
        
        return english_data
        
    except Exception as e:
        print(f"‚ùå Error during extraction: {e}")
        traceback.print_exc()
        return {}
    
    finally:
        driver.quit()


# ==========================================
# Health Check
# ==========================================
@app.get("/")
def health_check():
    return {"status": "API is running successfully"}


# ==========================================
# Manual Prediction
# ==========================================
@app.post("/predict")
def predict_price(input_data: ApartmentInput):
    try:
        df = prepare_input_dataframe(input_data.data)

        pred_log = model.predict(df)
        price_per_meter = float(np.exp(pred_log)[0])

        total_area = float(input_data.data.get("TotalArea", 0) or 0)
        total_price = price_per_meter * total_area

        return {
            "predicted_price_per_meter": round(price_per_meter, 2),
            "estimated_total_price": round(total_price, 2)
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# ==========================================
# Link-Based Prediction (Selenium Extraction)
# ==========================================
@app.post("/predict-from-link")
def predict_from_link(link_input: LinkInput):
    try:
        url = link_input.url.strip()

        if not url.startswith("http"):
            raise Exception("Invalid URL format")

        print("\n=== Opening Browser ===")
        extracted_data = extract_apartment_data(url)

        df = prepare_input_dataframe(extracted_data)

        print("=== Predicting ===")
        pred_log = model.predict(df)

        price_per_meter = float(np.exp(pred_log)[0])
        total_area = float(extracted_data.get("TotalArea", 0) or 0)
        total_price = price_per_meter * total_area

        return {
            "predicted_price_per_meter": round(price_per_meter, 2),
            "estimated_total_price": round(total_price, 2),
            "extracted_features": extracted_data
        }

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))